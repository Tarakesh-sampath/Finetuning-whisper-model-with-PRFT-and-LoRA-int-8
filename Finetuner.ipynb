{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: huggingface_hub in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (0.26.3)\n",
      "Requirement already satisfied: datasets in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: transformers in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (4.47.0.dev0)\n",
      "Requirement already satisfied: librosa in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: evaluate in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: jiwer in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: accelerate in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: bitsandbytes in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (0.44.1)\n",
      "Requirement already satisfied: gradio in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (4.44.1)\n",
      "Requirement already satisfied: loralib in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: filelock in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from datasets) (3.11.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: audioread>=2.1.9 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from jiwer) (3.10.1)\n",
      "Requirement already satisfied: psutil in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from accelerate) (2.5.1+cu124)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (4.6.2.post1)\n",
      "Requirement already satisfied: fastapi<1.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (0.115.5)\n",
      "Requirement already satisfied: ffmpy in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (0.28.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (6.4.5)\n",
      "Requirement already satisfied: jinja2<4.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (3.9.2)\n",
      "Requirement already satisfied: orjson~=3.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (3.10.12)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (10.2.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (2.10.2)\n",
      "Requirement already satisfied: pydub in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (0.0.18)\n",
      "Requirement already satisfied: ruff>=0.2.2 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (0.8.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (0.14.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio) (0.32.1)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: colorama in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from fastapi<1.0->gradio) (0.41.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: certifi in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.21.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: networkx in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: pycparser in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/huggingface/peft\n",
      "  Cloning https://github.com/huggingface/peft to c:\\users\\tarak\\appdata\\local\\temp\\pip-req-build-0tvyi9m5\n",
      "  Resolved https://github.com/huggingface/peft to commit 3f9ce553e21569e21269b5ba91d7390f7229199a\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from peft==0.13.3.dev0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from peft==0.13.3.dev0) (24.2)\n",
      "Requirement already satisfied: psutil in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from peft==0.13.3.dev0) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from peft==0.13.3.dev0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from peft==0.13.3.dev0) (2.5.1+cu124)\n",
      "Requirement already satisfied: transformers in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from peft==0.13.3.dev0) (4.47.0.dev0)\n",
      "Requirement already satisfied: tqdm in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from peft==0.13.3.dev0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from peft==0.13.3.dev0) (1.1.1)\n",
      "Requirement already satisfied: safetensors in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from peft==0.13.3.dev0) (0.4.5)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from peft==0.13.3.dev0) (0.26.3)\n",
      "Requirement already satisfied: filelock in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft==0.13.3.dev0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft==0.13.3.dev0) (2024.9.0)\n",
      "Requirement already satisfied: requests in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft==0.13.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from huggingface_hub>=0.25.0->peft==0.13.3.dev0) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from torch>=1.13.0->peft==0.13.3.dev0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from torch>=1.13.0->peft==0.13.3.dev0) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from torch>=1.13.0->peft==0.13.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.13.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from tqdm->peft==0.13.3.dev0) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from transformers->peft==0.13.3.dev0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from transformers->peft==0.13.3.dev0) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.13.3.dev0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft==0.13.3.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft==0.13.3.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft==0.13.3.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\work\\vs_code_doc\\repo\\finetuning-whisper-model-with-prft-and-lora-int-8\\venv\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft==0.13.3.dev0) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft 'C:\\Users\\tarak\\AppData\\Local\\Temp\\pip-req-build-0tvyi9m5'\n"
     ]
    }
   ],
   "source": [
    "!pip3 install huggingface_hub datasets transformers librosa evaluate jiwer accelerate bitsandbytes gradio loralib\n",
    "!pip3 install git+https://github.com/huggingface/peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "language = \"English\"\n",
    "language_abbr = \"en\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"Tarakeshwaran/Whisper-train-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'text', 'start', 'end'],\n",
      "        num_rows: 80\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'text', 'start', 'end'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "data_dict= DatasetDict()\n",
    "\n",
    "data_dict[\"train\"] = load_dataset(dataset_name, split=\"train\")\n",
    "data_dict[\"test\"] = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_dict.remove_columns([\"start\",\"end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'text'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Feature Extractor, Tokenizer and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'sample-005811.mp3', 'array': array([-3.10192730e-25,  8.27180613e-25, -4.65289095e-24, ...,\n",
      "       -1.07697597e-05,  2.15554501e-05, -2.88033916e-05]), 'sampling_rate': 16000}, 'text': \"in alchemy it's called the soul of the world\"}\n"
     ]
    }
   ],
   "source": [
    "print(data_dict[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_dict.map(prepare_dataset, remove_columns=data_dict.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_features', 'labels'],\n",
       "    num_rows: 80\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load pretrained check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,728,640 || all params: 1,559,033,600 || trainable%: 1.0089\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"temp\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=25,\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tarak\\AppData\\Local\\Temp\\ipykernel_5888\\3794436891.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=data_dict[\"train\"],\n",
    "    eval_dataset=data_dict[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.data = param.data.half()  # or .float() depending on the primary precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [10:59<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: struct c10::Half != float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\transformers\\trainer.py:2167\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2165\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\transformers\\trainer.py:2525\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2519\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2520\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2522\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2523\u001b[0m )\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2525\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2528\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2530\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2531\u001b[0m ):\n\u001b[0;32m   2532\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2533\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\transformers\\trainer.py:3688\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3686\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3689\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\accelerate\\accelerator.py:2237\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2238\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\torch\\autograd\\function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\torch\\utils\\checkpoint.py:321\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[1;34m(ctx, *args)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone of output has requires_grad=True,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this checkpoint() is not necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m     )\n\u001b[1;32m--> 321\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    323\u001b[0m     inp\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs\n\u001b[0;32m    325\u001b[0m )\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m grads\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\torch\\autograd\\function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\vs_code_doc\\repo\\Finetuning-whisper-model-with-PRFT-and-LoRA-int-8\\venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:474\u001b[0m, in \u001b[0;36mMatMul8bitLt.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    473\u001b[0m     CB \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mCB\u001b[38;5;241m.\u001b[39mto(ctx\u001b[38;5;241m.\u001b[39mdtype_A, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmul_(state\u001b[38;5;241m.\u001b[39mSCB\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m127.0\u001b[39m))\n\u001b[1;32m--> 474\u001b[0m     grad_A \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCB\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(ctx\u001b[38;5;241m.\u001b[39mgrad_shape)\u001b[38;5;241m.\u001b[39mto(ctx\u001b[38;5;241m.\u001b[39mdtype_A)\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mCxB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m     CB \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    477\u001b[0m         undo_layout(state\u001b[38;5;241m.\u001b[39mCxB, state\u001b[38;5;241m.\u001b[39mtile_indices)\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;241m.\u001b[39mto(ctx\u001b[38;5;241m.\u001b[39mdtype_A)\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;241m.\u001b[39mmul_(state\u001b[38;5;241m.\u001b[39mSCB\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m127.0\u001b[39m))\n\u001b[0;32m    480\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: struct c10::Half != float"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "peft_model_id = \"Tarakeshwaran/\" + f\"{model_name_or_path}-{model.peft_config.peft_type}-colab\".replace(\"/\", \"-\")\n",
    "model.push_to_hub(peft_model_id)\n",
    "print(peft_model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
